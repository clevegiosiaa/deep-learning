{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "\n",
    "**Notasi**:\n",
    "- Superscript $[l]$ menunjukkan objek yang terkait dengan lapisan $l^{th}$.\n",
    "\n",
    "- Superscript $(i)$ menunjukkan objek yang terkait dengan data ke $i^{th}$.\n",
    "\n",
    "- Superscript $\\langle t \\rangle$ menunjukkan suatu objek pada langkah $t^{th}$.\n",
    "    \n",
    "- Subscript $i$ menunjukkan entri $i^{th}$ dari sebuah vektor.\n",
    "\n",
    "**Contoh**:\n",
    "- $a^{(2)[3]<4>}_5$ menunjukkan aktivasi contoh pelatihan ke-2 (2), lapisan ke-3 [3], langkah ke-4 <4>, dan entri ke-5 dalam vektor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lyejzf2TELFr"
   },
   "source": [
    "<a name='0'></a>\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXWmy3GdELFr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn_utils import *\n",
    "from public_tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "<a name='1'></a>\n",
    "## Propagasi Forward untuk Recurrent Neural Network Dasar\n",
    "\n",
    "RNN dasar yang akan Anda terapkan memiliki struktur berikut:\n",
    "\n",
    "Dalam contoh ini, $T_x = T_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZBxzkRzsELFv"
   },
   "source": [
    "<img src=\"images/RNN.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Gambar 1</b>: RNN model </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "### Dimensi input $x$\n",
    "\n",
    "#### Input dengan jumlah unit $n_x$\n",
    "* Untuk sekali langkah dari satu data input, $x^{(i) \\langle t \\rangle }$ adalah vektor input satu dimensi\n",
    "* Dengan menggunakan bahasa sebagai contoh, bahasa dengan vocabulary 5000 kata dapat diberlakukan encoding secara one-hot ke dalam vektor yang memiliki 5000 unit. Jadi $x^{(i)\\langle t \\rangle}$ akan berbentuk (5000,)\n",
    "* Notasi $n_x$ digunakan di sini untuk menunjukkan jumlah unit dalam satu langkah dari satu data latih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Langkah pindah berukuran $T_{x}$\n",
    "* Recurrent neural network memiliki beberapa langkah, yang akan Anda indeks dengan $t$.\n",
    "* Dalam pelajaran, Anda melihat satu data latih $x^{(i)}$ yang terdiri dari beberapa langkah pindah $T_x$. Di notebook ini, $T_{x}$ akan menunjukkan jumlah langkah pindah dalam urutan terpanjang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Batch ukuran $m$\n",
    "* Katakanlah kita memiliki mini-batches, masing-masing berisi 20 data latih\n",
    "* Untuk mendapatkan manfaat dari vektorisasi, Anda akan menumpuk 20 kolom data $x^{(i)}$\n",
    "* Misalnya tensor ini berbentuk (5000,20,10)\n",
    "* Anda akan menggunakan $m$ untuk menunjukkan jumlah data latih\n",
    "* Jadi, bentuk mini-batchnya adalah $(n_x,m,T_x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Tensor 3D berbentuk $(n_{x},m,T_{x})$\n",
    "*Tensor $x$ 3 dimensi berbentuk $(n_x,m,T_x)$ mewakili input $x$ yang dimasukkan ke RNN\n",
    "\n",
    "#### Mengambil potongan 2D untuk setiap langkah pindah: $x^{\\langle t \\rangle}$\n",
    "* Pada setiap langkah pindah, Anda akan menggunakan mini-batch data latih (bukan hanya satu contoh)\n",
    "* Jadi, untuk setiap langkah pindah $t$, Anda akan menggunakan potongan 2D bentuk $(n_x,m)$\n",
    "* Irisan 2D ini disebut sebagai $x^{\\langle t \\rangle}$. Nama variabel dalam code adalah `xt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "### Definisi hidden state $a$\n",
    "\n",
    "* Aktivasi $a^{\\langle t \\rangle}$ yang diteruskan ke RNN dari satu langkah pindah ke langkah lainnya disebut \"hidden state\".\n",
    "\n",
    "### Dimensi hidden state $a$\n",
    "\n",
    "* Mirip dengan input tensor $x$, hidden state untuk data latih tunggal adalah vektor dengan panjang $n_{a}$\n",
    "* Jika Anda menyertakan mini-batch dari data latih berukuran $m$, bentuk mini-batch adalah $(n_{a},m)$\n",
    "* Saat Anda memasukkan dimensi langkah pindah, bentuk hidden statenya adalah $(n_{a}, m, T_x)$\n",
    "* Anda akan mengulangi langkah-langkah waktu dengan indeks $t$, dan bekerja dengan potongan 2D dari tensor 3D\n",
    "* Irisan 2D ini disebut sebagai $a^{\\langle t \\rangle}$\n",
    "* Dalam code, nama variabel yang digunakan adalah `a_prev` atau `a_next`, bergantung pada fungsi yang diimplementasikan\n",
    "* Bentuk irisan 2D ini adalah $(n_{a}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "### Dimensi prediksi $\\hat{y}$\n",
    "* Mirip dengan input dan hidden state, $\\hat{y}$ adalah tensor 3D berbentuk $(n_{y}, m, T_{y})$\n",
    "* $n_{y}$: jumlah unit dalam vektor yang mewakili prediksi\n",
    "* $m$: jumlah data dalam mini-batch\n",
    "* $T_{y}$: jumlah langkah pindah dalam prediksi\n",
    "* Untuk satu langkah pindah $t$, irisan 2D $\\hat{y}^{\\langle t \\rangle}$ memiliki bentuk $(n_{y}, m)$\n",
    "* Di dalam code, nama variabelnya adalah:\n",
    "    - `y_pred`: $\\hat{y}$\n",
    "    - `yt_pred`: $\\hat{y}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "Cara mengimplementasikan RNN:\n",
    "\n",
    "### Langkah:\n",
    "1. implementasi penghitungan yang diperlukan untuk satu langkah pindah RNN.\n",
    "2. implementasi loop untuk setiap langkah pindah $T_x$ untuk memproses semua input, satu per satu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "<a name='1-1'></a>\n",
    "### RNN cell\n",
    "\n",
    "Anda dapat menganggap recurrent neural network sebagai penggunaan berulang dari satu sel. Pertama, Anda akan menerapkan komputasi untuk satu langkah pindah. Gambar berikut menjelaskan operasi untuk satu langkah waktu sel RNN:\n",
    "\n",
    "<img src=\"images/rnn_step_forward_figure2_v3a.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Gambar 2</b>: Sel RNN dasar. Mengambil input $x^{\\langle t \\rangle}$ (input saat ini) dan $a^{\\langle t - 1\\rangle}$ (hidden state sebelumnya yang berisi informasi sebelumnya), dan menghasilkan output $a^{\\langle t \\rangle}$ yang diberikan ke sel RNN berikutnya dan juga digunakan untuk memprediksi $\\hat{y}^{\\langle t \\rangle}$\n",
    "</center></caption>\n",
    "\n",
    "**`RNN cell` versus `RNN_cell_forward`**:\n",
    "* Perhatikan bahwa sel RNN mengeluarkan hidden state $a^{\\langle t \\rangle}$.\n",
    "* `RNN cell` ditampilkan pada gambar sebagai kotak sisi dalam dengan garis padat\n",
    "* Fungsi yang akan Anda implementasikan, `rnn_cell_forward`, juga menghitung prediksi $\\hat{y}^{\\langle t \\rangle}$\n",
    "* `RNN_cel_forward` ditampilkan pada gambar sebagai kotak sisi luar dengan garis putus-putus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "<a name='ex-1'></a>\n",
    "### Tugas 1 - rnn_cell_forward\n",
    "\n",
    "Implementasikan sel RNN yang dijelaskan pada Gambar.\n",
    "\n",
    "**Petunjuk**:\n",
    "1. Hitung hidden state dengan aktivasi tanh: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$\n",
    "2. Dengan menggunakan $a^{\\langle t \\rangle}$ hidden state baru, hitung prediksi $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. (Fungsi `softmax` disediakan)\n",
    "3. Simpan $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ dalam `cache`\n",
    "4. Return $a^{\\langle t \\rangle}$ , $\\hat{y}^{\\langle t \\rangle}$ dan `cache`\n",
    "\n",
    "#### Petunjuk Tambahan\n",
    "* Sedikit informasi lebih lanjut tentang [numpy.tanh](https://numpy.org/devdocs/reference/generated/numpy.tanh.html)\n",
    "* Dalam tugas ini, terdapat fungsi `softmax` yang dapat Anda gunakan. Itu terletak di file 'rnn_utils.py' dan telah diimpor.\n",
    "* Untuk perkalian matriks, gunakan [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxI-F0HWELF1"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: rnn_cell_forward\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    ### START CODE HERE ### (≈2 lines)\n",
    "    # compute next activation state using the formula given above\n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    # compute output of the current cell using the formula given above\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V03ZGazVELF4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] = \n",
      " [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape = \n",
      " (5, 10)\n",
      "yt_pred[1] =\n",
      " [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
      " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
      "yt_pred.shape = \n",
      " (2, 10)\n",
      "\u001b[92mAll tests passed\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3, 10)\n",
    "a_prev_tmp = np.random.randn(5, 10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Waa'] = np.random.randn(5, 5)\n",
    "parameters_tmp['Wax'] = np.random.randn(5, 3)\n",
    "parameters_tmp['Wya'] = np.random.randn(2, 5)\n",
    "parameters_tmp['ba'] = np.random.randn(5, 1)\n",
    "parameters_tmp['by'] = np.random.randn(2, 1)\n",
    "\n",
    "a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
    "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
    "print(\"a_next.shape = \\n\", a_next_tmp.shape)\n",
    "print(\"yt_pred[1] =\\n\", yt_pred_tmp[1])\n",
    "print(\"yt_pred.shape = \\n\", yt_pred_tmp.shape)\n",
    "\n",
    "# UNIT TESTS\n",
    "rnn_cell_forward_tests(rnn_cell_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_6KJp0lELF7"
   },
   "source": [
    "**Expected Output**: \n",
    "```Python\n",
    "a_next[4] = \n",
    " [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
    " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
    "a_next.shape = \n",
    " (5, 10)\n",
    "yt_pred[1] =\n",
    " [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
    "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
    "yt_pred.shape = \n",
    " (2, 10)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "<a name='1-2'></a>\n",
    "### RNN Forward Pass\n",
    "\n",
    "- Recurrent Neural Network (RNN) adalah repitisi sel RNN yang baru saja Anda buat.\n",
    "    - Jika urutan input data Anda panjangnya 10 langkah pindah, maka Anda akan menggunakan kembali sel RNN sebanyak 10 kali\n",
    "- Setiap sel mengambil dua input pada setiap langkah pindah:\n",
    "    - $a^{\\langle t-1 \\rangle}$: Hidden state dari sel sebelumnya\n",
    "    - $x^{\\langle t \\rangle}$: Data input langkah pindah saat ini\n",
    "- Ini memiliki dua output pada setiap langkah waktu:\n",
    "    - Hidden state ($a^{\\langle t \\rangle}$)\n",
    "    - Prediksi ($y^{\\langle t \\rangle}$)\n",
    "- Bobot dan bias $(W_{aa}, b_{a}, W_{ax}, b_{x})$ digunakan kembali setiap kali langkah pindah\n",
    "    - Mereka dipertahankan di antara panggilan ke `rnn_cell_forward` dalam dictionary 'parameter'\n",
    "\n",
    "\n",
    "<img src=\"images/rnn_forward_sequence_figure3_v3a.png\" style=\"width:800px;height:180px;\">\n",
    "<caption><center><font color='purple'><b>Gambar 3</b>: RNN Dasar. Urutan input $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ dibawa melalui langkah pindah $T_x$. Network mengeluarkan $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "<a name='ex-2'></a>\n",
    "### Tugas 2 - rnn_forward\n",
    "\n",
    "Menerapkan propagasi RNN forward yang dijelaskan pada Gambar 3.\n",
    "\n",
    "**Petunjuk**:\n",
    "* Buat array 3D nol, $a$ berbentuk $(n_{a}, m, T_{x})$ yang akan menyimpan semua hidden state yang dihitung oleh RNN\n",
    "* Buat array 3D nol, $\\hat{y}$, bentuk $(n_{y}, m, T_{x})$ yang akan menyimpan prediksi\n",
    "    - Perhatikan bahwa dalam kasus ini, $T_{y} = T_{x}$ (prediksi dan input memiliki jumlah langkah pindah yang sama)\n",
    "* Inisiasi hidden state 2D `a_next` dengan mengaturnya sama dengan hidden state awal, $a_{0}$\n",
    "* Pada setiap langkah pindah $t$:\n",
    "    - Dapatkan $x^{\\langle t \\rangle}$, yang merupakan potongan 2D dari $x$ untuk satu langkah pindah $t$\n",
    "        - $x^{\\langle t \\rangle}$ memiliki bentuk $(n_{x}, m)$\n",
    "        - $x$ memiliki bentuk $(n_{x}, m, T_{x})$\n",
    "    - Perbarui hidden state 2D $a^{\\langle t \\rangle}$ (nama variabel `a_next`), prediksi $\\hat{y}^{\\langle t \\rangle}$ dan cache dengan menjalankan `rnn_cell_forward`\n",
    "        - $a^{\\langle t \\rangle}$ memiliki bentuk $(n_{a}, m)$\n",
    "    - Simpan hidden state 2D di tensor 3D $a$, pada posisi $t^{th}$\n",
    "        - $a$ memiliki bentuk $(n_{a}, m, T_{x})$\n",
    "    - Simpan prediksi 2D $\\hat{y}^{\\langle t \\rangle}$ (nama variabel `yt_pred`) di tensor 3D $\\hat{y}_{pred}$ pada posisi $t^{th}$\n",
    "        - $\\hat{y}^{\\langle t \\rangle}$ memiliki bentuk $(n_{y}, m)$\n",
    "        - $\\hat{y}$ memiliki bentuk $(n_{y}, m, T_x)$\n",
    "    - Tambahkan cache ke daftar cache\n",
    "* Mengembalikan tensor 3D $a$ dan $\\hat{y}$, serta daftar cache\n",
    "\n",
    "#### Petunjuk Tambahan\n",
    "- Beberapa dokumentasi bermanfaat di [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html)\n",
    "- Jika Anda memiliki array numpy 3 dimensi dan mengindeks berdasarkan dimensi ketiganya, Anda dapat menggunakan slicing array seperti ini: `var_name[:,:,i]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VmeprGJpELF9"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: rnn_forward\n",
    "\n",
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_next = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Append \"cache\" to \"caches\" (≈1 line)\n",
    "        caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEPrd77rELF_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][1] = \n",
      " [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
      "a.shape = \n",
      " (5, 10, 4)\n",
      "y_pred[1][3] =\n",
      " [0.79560373 0.86224861 0.11118257 0.81515947]\n",
      "y_pred.shape = \n",
      " (2, 10, 4)\n",
      "caches[1][1][3] =\n",
      " [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
      "len(caches) = \n",
      " 2\n",
      "\u001b[92mAll tests passed\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x_tmp = np.random.randn(3, 10, 4)\n",
    "a0_tmp = np.random.randn(5, 10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Waa'] = np.random.randn(5, 5)\n",
    "parameters_tmp['Wax'] = np.random.randn(5, 3)\n",
    "parameters_tmp['Wya'] = np.random.randn(2, 5)\n",
    "parameters_tmp['ba'] = np.random.randn(5, 1)\n",
    "parameters_tmp['by'] = np.random.randn(2, 1)\n",
    "\n",
    "a_tmp, y_pred_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n",
    "print(\"a[4][1] = \\n\", a_tmp[4][1])\n",
    "print(\"a.shape = \\n\", a_tmp.shape)\n",
    "print(\"y_pred[1][3] =\\n\", y_pred_tmp[1][3])\n",
    "print(\"y_pred.shape = \\n\", y_pred_tmp.shape)\n",
    "print(\"caches[1][1][3] =\\n\", caches_tmp[1][1][3])\n",
    "print(\"len(caches) = \\n\", len(caches_tmp))\n",
    "\n",
    "#UNIT TEST    \n",
    "rnn_forward_test(rnn_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R135qjynELGC"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```Python\n",
    "a[4][1] = \n",
    " [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
    "a.shape = \n",
    " (5, 10, 4)\n",
    "y_pred[1][3] =\n",
    " [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
    "y_pred.shape = \n",
    " (2, 10, 4)\n",
    "caches[1][1][3] =\n",
    " [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
    "len(caches) = \n",
    " 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "<a name='2'></a>\n",
    "## Long Short-Term Memory (LSTM).\n",
    "\n",
    "Gambar berikut menunjukkan operasi sel LSTM:\n",
    "\n",
    "<img src=\"images/LSTM_figure4_v3a.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center><font color='purple'><b>Gambar 4</b>: sel LSTM. Ini melacak dan memperbarui \"cell state\", atau variabel memori $c^{\\langle t \\rangle}$ pada setiap langkah pindah, yang mungkin berbeda dari $a^{\\langle t \\rangle}$.\n",
    "Catatan, $softmax^{}$ menyertakan dense layer dan softmax.</center></caption>\n",
    "\n",
    "Mirip dengan contoh RNN di atas, Anda akan memulai dengan mengimplementasikan sel LSTM untuk satu langkah pindah. Kemudian, Anda akan memanggilnya secara iteratif dari dalam \"for loop\" agar ia memproses input dengan langkah pindah $T_x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "### Overview dari gates dan states\n",
    "\n",
    "#### Forget Gate $\\mathbf{\\Gamma}_{f}$\n",
    "\n",
    "* Anggaplah Anda membaca kata-kata dalam sebuah teks, dan berencana menggunakan LSTM untuk melacak struktur tata bahasa, seperti apakah subjeknya tunggal (\"anak anjing\") atau jamak (\"anak anjing\").\n",
    "* Jika subjek mengubah statenya (dari kata tunggal menjadi kata jamak), memori state sebelumnya menjadi usang, sehingga Anda akan \"melupakan\" state usang tersebut.\n",
    "* \"Forget Gate\" adalah tensor yang berisi nilai antara 0 dan 1.\n",
    "* Jika unit di forget gate memiliki nilai mendekati 0, LSTM akan \"melupakan\" status yang disimpan di unit terkait dari status sel sebelumnya.\n",
    "* Jika unit di forget gate memiliki nilai mendekati 1, sebagian besar LSTM akan mengingat nilai terkait dalam keadaan tersimpan.\n",
    "\n",
    "##### Persamaan\n",
    "\n",
    "$$\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)\\tag{1} $$\n",
    "\n",
    "\n",
    "##### Penjelasan persamaan:\n",
    "* $\\mathbf{W_{f}}$ berisi bobot yang mengatur perilaku forget gate.\n",
    "* $[a^{\\langle t-1 \\rangle}$ hidden state langkah pindah sebelumnya dan input $x^{\\langle t \\rangle}]$ langkah pindah saat ini digabungkan dan dikalikan dengan $\\mathbf{W_{f}}$.\n",
    "* Fungsi sigmoid digunakan untuk membuat setiap nilai tensor gate $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$ berkisar dari 0 hingga 1.\n",
    "* Forget gate $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$ mempunyai dimensi yang sama dengan state sel $c^{\\langle t-1 \\rangle}$ sebelumnya.\n",
    "* Artinya keduanya dapat dikalikan berdasarkan elemen.\n",
    "* Mengalikan tensor $\\mathbf{\\Gamma}_f^{\\langle t \\rangle} * \\mathbf{c}^{\\langle t-1 \\rangle}$ seperti menerapkan penutup pada state sel sebelumnya.\n",
    "* Jika nilai tunggal di $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$ adalah 0 atau mendekati 0, maka hasil kali mendekati 0.\n",
    "* Ini menjaga informasi yang disimpan dalam unit terkait di $\\mathbf{c}^{\\langle t-1 \\rangle}$ agar tidak diingat untuk langkah waktu berikutnya.\n",
    "* Demikian pula, jika salah satu nilai mendekati 1, hasil perkaliannya mendekati nilai asli pada state sel sebelumnya.\n",
    "* LSTM akan menyimpan informasi dari unit $\\mathbf{c}^{\\langle t-1 \\rangle}$ yang sesuai, untuk digunakan pada langkah pindah berikutnya.\n",
    "    \n",
    "##### Nama variabel dalam code\n",
    "Nama variabel dalam code mirip dengan persamaan, dengan sedikit perbedaan.\n",
    "* `Wf`: forget gate weight $\\mathbf{W}_{f}$\n",
    "*`bf`: forget gate bias $\\mathbf{b}_{f}$\n",
    "* `ft`: forget gate $\\Gamma_f^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Nilai kandidat $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$\n",
    "* Nilai kandidat adalah tensor yang berisi informasi dari langkah pindah saat ini yang **mungkin** disimpan dalam state sel saat ini $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "* Bagian dari nilai kandidat yang diteruskan bergantung pada update gate.\n",
    "* Nilai kandidat adalah tensor yang berisi nilai yang berkisar dari -1 hingga 1.\n",
    "* Tanda tilde \"~\" digunakan untuk membedakan kandidat $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ dari state sel $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "\n",
    "##### Persamaan\n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right) \\tag{3}$$\n",
    "\n",
    "\n",
    "##### Penjelasan persamaan\n",
    "* Fungsi *tanh* menghasilkan nilai antara -1 dan 1.\n",
    "\n",
    "\n",
    "##### Nama variabel dalam kode\n",
    "* `cct`: nilai kandidat $\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Update Gate $\\mathbf{\\Gamma}_{i}$\n",
    "\n",
    "* Anda menggunakan update gate untuk memutuskan aspek kandidat $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ mana yang akan ditambahkan ke state sel $c^{\\langle t \\rangle}$.\n",
    "* Update gate memutuskan bagian mana dari tensor \"kandidat\" $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ yang diteruskan ke state sel $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "* Update gate adalah tensor yang berisi nilai antara 0 dan 1.\n",
    "* Ketika unit di update gate mendekati 1, ini memungkinkan nilai kandidat $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ diteruskan ke hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "* Ketika unit di update gate mendekati 0, hal ini mencegah nilai terkait dalam kandidat diteruskan ke hidden state.\n",
    "* Perhatikan bahwa subscript \"i\" digunakan dan bukan \"u\", untuk mengikuti konvensi yang digunakan dalam literatur.\n",
    "\n",
    "##### Persamaan\n",
    "\n",
    "$$\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2} $$ \n",
    "\n",
    "##### Penjelasan persamaan\n",
    "\n",
    "* Mirip dengan forget gate, di sini $\\mathbf{\\Gamma}_i^{\\langle t \\rangle}$, sigmoid menghasilkan nilai antara 0 dan 1.\n",
    "* Update gate dikalikan berdasarkan elemen dengan kandidat, dan hasil kali ini ($\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} * \\tilde{c}^{\\langle t \\rangle}$) digunakan dalam menentukan state sel $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "\n",
    "##### Nama variabel dalam code (Harap diperhatikan bahwa nama variabel berbeda dengan persamaan)\n",
    "Dalam code tersebut, Anda akan menggunakan nama variabel yang ditemukan dalam literatur akademis. Variabel-variabel ini tidak menggunakan \"u\" untuk menunjukkan \"update\".\n",
    "* `Wi` adalah update gate weight $\\mathbf{W}_i$ (bukan \"Wu\")\n",
    "* `bi` adalah update gate bias $\\mathbf{b}_i$ (bukan \"bu\")\n",
    "* `itu` adalah update gate $\\mathbf{\\Gamma}_i^{\\langle t \\rangle}$ (bukan \"ut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Cell state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "\n",
    "* cell state adalah \"memori\" yang diteruskan ke langkah waktu mendatang.\n",
    "* state cell baru $\\mathbf{c}^{\\langle t \\rangle}$ adalah kombinasi dari state cell sebelumnya dan nilai kandidat.\n",
    "\n",
    "##### Persamaan\n",
    "\n",
    "$$ \\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle} \\tag{4} $$\n",
    "\n",
    "##### Penjelasan persamaan\n",
    "*Keadaan sel sebelumnya $\\mathbf{c}^{\\langle t-1 \\rangle}$ disesuaikan (weighted) oleh forget gate $\\mathbf{\\Gamma}_{f}^{\\langle t \\rangle}$* dan nilai kandidat $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$, disesuaikan (weighted) dengan update gate $\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle}$\n",
    "\n",
    "##### Nama dan bentuk variabel dalam kode\n",
    "* `c`: cell state, termasuk semua langkah pindah, bentuk $\\mathbf{c}$ $(n_{a}, m, T_x)$\n",
    "* `c_next`: cell state baru (berikutnya), bentuk $\\mathbf{c}^{\\langle t \\rangle}$ $(n_{a}, m)$\n",
    "* `c_prev`: cell_state sebelumnya, $\\mathbf{c}^{\\langle t-1 \\rangle}$, bentuk $(n_{a}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Output gate $\\mathbf{\\Gamma}_{o}$\n",
    "\n",
    "* output gate memutuskan apa yang dikirim sebagai prediksi (output) dari suatu langkah pindah.\n",
    "* output gate sama seperti gate lainnya, di dalamnya berisi nilai yang berkisar dari 0 hingga 1.\n",
    "\n",
    "##### Persamaan\n",
    "\n",
    "$$ \\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})\\tag{5}$$ \n",
    "\n",
    "##### Penjelasan persamaan\n",
    "* output gate ditentukan oleh hidden state sebelumnya $\\mathbf{a}^{\\langle t-1 \\rangle}$ dan input saat ini $\\mathbf{x}^{\\langle t \\rangle}$\n",
    "* Sigmoid membuat gate berkisar dari 0 hingga 1.\n",
    "\n",
    "\n",
    "##### Nama variabel dalam kode\n",
    "* `Wo`: output gate weight, $\\mathbf{W_o}$\n",
    "* `bo`: output gate bias, $\\mathbf{b_o}$\n",
    "* `ot`: output gate, $\\mathbf{\\Gamma}_{o}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Hidden State $\\mathbf{a}^{\\langle t \\rangle}$\n",
    "\n",
    "* Hidden state diteruskan ke langkah pindah sel LSTM berikutnya.\n",
    "* Digunakan untuk menentukan tiga gate ($\\mathbf{\\Gamma}_{f}, \\mathbf{\\Gamma}_{u}, \\mathbf{\\Gamma}_{o}$) langkah pindah berikutnya.\n",
    "* Hidden state juga digunakan untuk prediksi $y^{\\langle t \\rangle}$.\n",
    "\n",
    "##### Persamaan\n",
    "\n",
    "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "##### Penjelasan persamaan\n",
    "* Hidden state $\\mathbf{a}^{\\langle t \\rangle}$ ditentukan oleh state sel $\\mathbf{c}^{\\langle t \\rangle}$ yang dikombinasikan dengan output gate $\\mathbf{\\Gamma}_{o}$.\n",
    "* State sel diteruskan melalui fungsi `tanh` untuk mengubah skala nilai antara -1 dan 1.\n",
    "* Output gate bertindak seperti \"mask\" yang mempertahankan nilai-nilai $\\tanh(\\mathbf{c}^{\\langle t \\rangle})$ atau menjaga nilai-nilai tersebut agar tidak dimasukkan dalam hidden state $\\mathbf{a}^{\\langle t \\rangle}$\n",
    "\n",
    "##### Nama dan bentuk variabel dalam code\n",
    "* `a`: hidden state, termasuk langkah pindahnya. $\\mathbf{a}$ memiliki bentuk $(n_{a}, m, T_{x})$\n",
    "* `a_prev`: hidden state dari langkah pindah sebelumnya. $\\mathbf{a}^{\\langle t-1 \\rangle}$ memiliki bentuk $(n_{a}, m)$\n",
    "* `a_next`: hidden state untuk langkah pindah berikutnya. $\\mathbf{a}^{\\langle t \\rangle}$ memiliki bentuk $(n_{a}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7OYaNPNPELGH",
    "lang": "en"
   },
   "source": [
    "#### Prediction $\\mathbf{y}^{\\langle t \\rangle}_{pred}$\n",
    "* The prediction in this use case is a classification, so you'll use a softmax.\n",
    "\n",
    "The equation is:\n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$\n",
    "\n",
    "##### Variable names and shapes in the code\n",
    "* `y_pred`: prediction, including all time steps. $\\mathbf{y}_{pred}$ has shape $(n_{y}, m, T_{x})$.  Note that $(T_{y} = T_{x})$ for this example.\n",
    "* `yt_pred`: prediction for the current time step $t$. $\\mathbf{y}^{\\langle t \\rangle}_{pred}$ has shape $(n_{y}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Prediksi $\\mathbf{y}^{\\langle t \\rangle}_{pred}$\n",
    "* Prediksi dalam use case ini adalah klasifikasi, jadi Anda akan menggunakan softmax.\n",
    "\n",
    "Persamaannya adalah:\n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$\n",
    "\n",
    "##### Nama dan bentuk variabel dalam kode\n",
    "* `y_pred`: prediksi, termasuk semua langkah pindah. $\\mathbf{y}_{pred}$ memiliki bentuk $(n_{y}, m, T_{x})$. Perhatikan bahwa $(T_{y} = T_{x})$ untuk contoh ini.\n",
    "* `yt_pred`: prediksi untuk langkah pindah saat ini $t$. $\\mathbf{y}^{\\langle t \\rangle}_{pred}$ memiliki bentuk $(n_{y}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "<a name='2-1'></a>\n",
    "### LSTM Cell\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Tugas 3 - lstm_cell_forward\n",
    "\n",
    "Implementasikan sel LSTM yang dijelaskan pada Gambar 4.\n",
    "\n",
    "**Petunjuk**:\n",
    "1. Gabungkan hidden state $a^{\\langle t-1 \\rangle}$ dan input $x^{\\langle t \\rangle}$ ke dalam satu matriks:\n",
    "\n",
    "$$concat = \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$$  \n",
    "\n",
    "2. Hitung semua rumus (1 sampai 6) untuk gate, hidden state, dan cell state.\n",
    "3. Hitung prediksi $y^{\\langle t \\rangle}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "#### Petunjuk Tambahan\n",
    "* Anda dapat menggunakan [numpy.concatenate](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html). Periksa nilai mana yang akan digunakan untuk parameter `axis`.\n",
    "* Fungsi `sigmoid()` dan `softmax` diimpor dari `rnn_utils.py`.\n",
    "* Beberapa dokumen untuk [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html)\n",
    "* Gunakan [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) untuk perkalian matriks.\n",
    "* Perhatikan bahwa nama variabel `Wi`, `bi` mengacu pada bobot dan bias **update gate**. Tidak ada variabel bernama \"Wu\" atau \"bu\" dalam fungsi ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JU3tUxvmELGJ"
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: lstm_cell_forward\n",
    "\n",
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    c_next -- next memory state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    \n",
    "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
    "          c stands for the cell state (memory)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"] # forget gate weight\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"] # update gate weight (notice the variable name)\n",
    "    bi = parameters[\"bi\"] # (notice the variable name)\n",
    "    Wc = parameters[\"Wc\"] # candidate value weight\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"] # output gate weight\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"] # prediction weight\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Concatenate a_prev and xt (≈1 line)\n",
    "    concat = np.concatenate((a_prev, xt), axis=0)\n",
    "\n",
    "    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)\n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)\n",
    "    c_next = ft * c_prev + it * cct\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    \n",
    "    # Compute prediction of the LSTM cell (≈1 line)\n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9ssBEoxELGN",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] = \n",
      " [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
      "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
      "a_next.shape =  (5, 10)\n",
      "c_next[2] = \n",
      " [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
      "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
      "c_next.shape =  (5, 10)\n",
      "yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n",
      " 0.00943007 0.12666353 0.39380172 0.07828381]\n",
      "yt.shape =  (2, 10)\n",
      "cache[1][3] =\n",
      " [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
      "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
      "len(cache) =  10\n",
      "\u001b[92mAll tests passed\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3, 10)\n",
    "a_prev_tmp = np.random.randn(5, 10)\n",
    "c_prev_tmp = np.random.randn(5, 10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5 + 3)\n",
    "parameters_tmp['bf'] = np.random.randn(5, 1)\n",
    "parameters_tmp['Wi'] = np.random.randn(5, 5 + 3)\n",
    "parameters_tmp['bi'] = np.random.randn(5, 1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5 + 3)\n",
    "parameters_tmp['bo'] = np.random.randn(5, 1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5 + 3)\n",
    "parameters_tmp['bc'] = np.random.randn(5, 1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2, 5)\n",
    "parameters_tmp['by'] = np.random.randn(2, 1)\n",
    "\n",
    "a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)\n",
    "\n",
    "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
    "print(\"a_next.shape = \", a_next_tmp.shape)\n",
    "print(\"c_next[2] = \\n\", c_next_tmp[2])\n",
    "print(\"c_next.shape = \", c_next_tmp.shape)\n",
    "print(\"yt[1] =\", yt_tmp[1])\n",
    "print(\"yt.shape = \", yt_tmp.shape)\n",
    "print(\"cache[1][3] =\\n\", cache_tmp[1][3])\n",
    "print(\"len(cache) = \", len(cache_tmp))\n",
    "\n",
    "# UNIT TEST\n",
    "lstm_cell_forward_test(lstm_cell_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjSDYoQoELGP"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```Python\n",
    "a_next[4] = \n",
    " [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
    "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
    "a_next.shape =  (5, 10)\n",
    "c_next[2] = \n",
    " [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
    "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
    "c_next.shape =  (5, 10)\n",
    "yt[1] = [ 0.79913913  0.15986619  0.22412122  0.15606108  0.97057211  0.31146381\n",
    "  0.00943007  0.12666353  0.39380172  0.07828381]\n",
    "yt.shape =  (2, 10)\n",
    "cache[1][3] =\n",
    " [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
    "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
    "len(cache) =  10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "id"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### Forward Pass untuk LSTM\n",
    "\n",
    "Setelah mengimplementasikan satu langkah LSTM, Anda dapat mengulanginya menggunakan perulangan for untuk memproses rangkaian input $T_x$.\n",
    "\n",
    "<img src=\"images/LSTM_rnn.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Gambar 5</b>: LSTM dalam beberapa langkah pindah. </center></caption>\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Tugas 4 - lstm_forward\n",
    "    \n",
    "Implementasi `lstm_forward()` untuk menjalankan LSTM melalui langkah pindah $T_x$.\n",
    "\n",
    "**Petunjuk**\n",
    "* Dapatkan dimensi $n_x, n_a, n_y, m, T_x$ dari bentuk variabel: `x` dan `parameter`* Inisiasi tensor 3D $a$, $c$, dan $y$\n",
    "    - $a$: hidden state, bentuk $(n_{a}, m, T_{x})$\n",
    "    - $c$: cell state, bentuk $(n_{a}, m, T_{x})$\n",
    "    - $y$: prediksi, bentuk $(n_{y}, m, T_{x})$ (Perhatikan bahwa $T_{y} = T_{x}$ dalam contoh ini)\n",
    "    - **Catatan** Menyetel satu variabel sama dengan variabel lainnya adalah \"copy by reference\". Dengan kata lain, jangan lakukan `c = a', jika tidak, kedua variabel ini menunjuk ke variabel dasar yang sama.\n",
    "*Inisiasi tensor 2D $a^{\\langle t \\rangle}$\n",
    "    - $a^{\\langle t \\rangle}$ menyimpan hidden state untuk langkah pindah $t$. Nama variabelnya adalah `a_next`.\n",
    "    - $a^{\\langle 0 \\rangle}$, hidden state awal pada langkah waktu 0, diteruskan saat memanggil fungsi. Nama variabelnya adalah `a0`.\n",
    "    - $a^{\\langle t \\rangle}$ dan $a^{\\langle 0 \\rangle}$ mewakili satu langkah pindah, sehingga keduanya memiliki bentuk $(n_{a}, m)$\n",
    "    - Inisiasi $a^{\\langle t \\rangle}$ dengan mengaturnya ke hidden state awal ($a^{\\langle 0 \\rangle}$) yang diteruskan ke fungsi.\n",
    "* Inisiasi $c^{\\langle t \\rangle}$ dengan nol.\n",
    "    - Nama variabelnya adalah `c_next`\n",
    "    - $c^{\\langle t \\rangle}$ mewakili satu langkah pindah, jadi bentuknya adalah $(n_{a}, m)$\n",
    "    - **Catatan**: buat `c_next` sebagai variabelnya sendiri dengan lokasinya sendiri di memori. Jangan inisiasi sebagai bagian dari tensor 3D $c$. Dengan kata lain, **jangan** lakukan `c_next = c[:,:,0]`.\n",
    "* Untuk setiap langkah waktu, lakukan hal berikut:\n",
    "    - Dari tensor 3D $x$, dapatkan irisan 2D $x^{\\langle t \\rangle}$ pada langkah waktu $t$\n",
    "    - Panggil fungsi `lstm_cell_forward` yang Anda tentukan sebelumnya, untuk mendapatkan hidden state, cell state, prediksi, dan cache\n",
    "    - Simpan hidden state, cell state, dan prediksi (tensor 2D) di dalam tensor 3D\n",
    "    - Tambahkan cache ke daftar cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XMmJrPSdELGQ"
   },
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: lstm_forward\n",
    "\n",
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (4).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    Wy = parameters['Wy'] # saving parameters['Wy'] in a local variable in case students use Wy instead of parameters['Wy']\n",
    "    # Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wy\"].shape\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros (≈3 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next and c_next (≈2 lines)\n",
    "    a_next = a0\n",
    "    c_next = np.zeros(a_next.shape)\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
    "        xt = x[:,:,t]\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the next cell state (≈1 line)\n",
    "        c[:,:,t]  = c_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y[:,:,t] = yt\n",
    "        # Append the cache into caches (≈1 line)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JehC5gwdELGS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][3][6] =  0.1721177675329167\n",
      "a.shape =  (5, 10, 7)\n",
      "y[1][4][3] = 0.9508734618501101\n",
      "y.shape =  (2, 10, 7)\n",
      "caches[1][1][1] =\n",
      " [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
      "  0.41005165]\n",
      "c[1][2][1] -0.8555449167181983\n",
      "len(caches) =  2\n",
      "\u001b[92mAll tests passed\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x_tmp = np.random.randn(3, 10, 7)\n",
    "a0_tmp = np.random.randn(5, 10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5 + 3)\n",
    "parameters_tmp['bf'] = np.random.randn(5, 1)\n",
    "parameters_tmp['Wi'] = np.random.randn(5, 5 + 3)\n",
    "parameters_tmp['bi']= np.random.randn(5, 1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5 + 3)\n",
    "parameters_tmp['bo'] = np.random.randn(5, 1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5 + 3)\n",
    "parameters_tmp['bc'] = np.random.randn(5, 1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2, 5)\n",
    "parameters_tmp['by'] = np.random.randn(2, 1)\n",
    "\n",
    "a_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)\n",
    "print(\"a[4][3][6] = \", a_tmp[4][3][6])\n",
    "print(\"a.shape = \", a_tmp.shape)\n",
    "print(\"y[1][4][3] =\", y_tmp[1][4][3])\n",
    "print(\"y.shape = \", y_tmp.shape)\n",
    "print(\"caches[1][1][1] =\\n\", caches_tmp[1][1][1])\n",
    "print(\"c[1][2][1]\", c_tmp[1][2][1])\n",
    "print(\"len(caches) = \", len(caches_tmp))\n",
    "\n",
    "# UNIT TEST    \n",
    "lstm_forward_test(lstm_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAETfQVFELGV"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```Python\n",
    "a[4][3][6] =  0.172117767533\n",
    "a.shape =  (5, 10, 7)\n",
    "y[1][4][3] = 0.95087346185\n",
    "y.shape =  (2, 10, 7)\n",
    "caches[1][1][1] =\n",
    " [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
    "  0.41005165]\n",
    "c[1][2][1] -0.855544916718\n",
    "len(caches) =  2\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "DLSC5W1-A1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "id"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "id",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Notebook ini terdiri dari 2 bagian: persiapan data dan model continuous bag-of-words (CBOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji==1.4.1\n",
      "  Downloading emoji-1.4.1.tar.gz (185 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 185 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.4.1-py3-none-any.whl size=186394 sha256=a3500354171187e3e5a3735626ca9c993df2fd3c10956f06685ae418a6770482\n",
      "  Stored in directory: /Users/hendriksugiarto/Library/Caches/pip/wheels/66/98/c2/683c7cb1a5449f5d0936d0b65fe1ddd5ebae8e45638a0cd5c0\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "  Attempting uninstall: emoji\n",
      "    Found existing installation: emoji 2.8.0\n",
      "    Uninstalling emoji-2.8.0:\n",
      "      Successfully uninstalled emoji-2.8.0\n",
      "Successfully installed emoji-1.4.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install emoji==1.4.1\n",
    "#!{sys.executable} -m pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hendriksugiarto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "import numpy as np\n",
    "\n",
    "from utils2 import get_dict\n",
    "\n",
    "nltk.download('punkt')  # download pre-trained Punkt tokenizer for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagian ini terdiri dari:\n",
    "- Membersihkan dan tokenisasi corpus\n",
    "- Mempersiapkan context words dan center word untuk training data set CBOW. \n",
    "- Menciptakan representasi vektor sederhana dari context words (features) and center words (targets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a corpus\n",
    "corpus = 'Who ‚ù§Ô∏è \"word embeddings\" in 2022? I do üôÇ !!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  Who ‚ù§Ô∏è \"word embeddings\" in 2022? I do üôÇ !!!\n",
      "After cleaning punctuation:  Who ‚ù§Ô∏è \"word embeddings\" in 2022. I do üôÇ .\n"
     ]
    }
   ],
   "source": [
    "print(f'Corpus:  {corpus}') # Print original corpus\n",
    "data = re.sub(r'[,!?;-]+', '.', corpus) # Do the substitution\n",
    "print(f'After cleaning punctuation:  {data}') # Print cleaned corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunakan NLTK's tokenization engine untuk memisahkan corpus menjadi individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial string:  Who ‚ù§Ô∏è \"word embeddings\" in 2022. I do üôÇ .\n",
      "After tokenization:  ['Who', '‚ù§Ô∏è', '``', 'word', 'embeddings', \"''\", 'in', '2022', '.', 'I', 'do', 'üôÇ', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial string:  {data}') # Print cleaned corpus\n",
    "data = nltk.word_tokenize(data) # Tokenize the cleaned corpus\n",
    "print(f'After tokenization:  {data}') # Print the tokenized version of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buang angka dan tanda baca (selain titik), lalu ubah semua ke huruf kecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial list of tokens:  ['Who', '‚ù§Ô∏è', '``', 'word', 'embeddings', \"''\", 'in', '2022', '.', 'I', 'do', 'üôÇ', '.']\n",
      "After cleaning:  ['who', '‚ù§Ô∏è', 'word', 'embeddings', 'in', '.', 'i', 'do', 'üôÇ', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial list of tokens:  {data}') # Print the tokenized version of the corpus\n",
    "\n",
    "# Filter tokenized corpus using list comprehension\n",
    "data = [ ch.lower() for ch in data\n",
    "         if ch.isalpha()\n",
    "         or ch == '.'\n",
    "         or emoji.get_emoji_regexp().search(ch)\n",
    "       ]\n",
    "print(f'After cleaning:  {data}') # Print the tokenized and filtered version of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "\n",
    "Buatlah fungsi yang berisi semua proses cleaning dan tokenization di atas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'tokenize' function that will include the steps previously seen\n",
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]+', '.', corpus) # Do the substitution\n",
    "    data = nltk.word_tokenize(data) # Tokenize the cleaned corpus\n",
    "    data = [ ch.lower() for ch in data\n",
    "         if ch.isalpha()\n",
    "         or ch == '.'\n",
    "         or emoji.get_emoji_regexp().search(ch)\n",
    "       ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cobalah fungsi diatas pada kalimat: \"I am happy because I am learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  I am happy because I¬†am learning\n",
      "Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
     ]
    }
   ],
   "source": [
    "corpus = 'I am happy because I¬†am learning' # Define new corpus\n",
    "print(f'Corpus:  {corpus}') # Print new corpus\n",
    "words = tokenize(corpus) # Save tokenized version of corpus into 'words' variable\n",
    "print(f'Words (tokens):  {words}') # Print the tokenized version of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cobalah dengan menggunakan kalimatmu sendiri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mari', 'menulis', 'kata', 'apapun', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Mari menulis kata apapun ::: !\") # Run this with any sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding window of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang kamu dapat mencoba menggeser window of words. Untuk setiap window, carilah center word dan context words.\n",
    "\n",
    "### Tugas\n",
    "Buatlah fungsi `get_windows` yang berisi operasi ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'get_windows' function\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i-C):i]+words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\n",
    "for x, y in get_windows(\n",
    "            ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n",
    "            2\n",
    "        ):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contoh pertama terdiri dari \n",
    "- context words: \"i\", \"am\", \"because\", \"i\",\n",
    "- center word: \"happy\".\n",
    "\n",
    "Cobalah gunakan kata-katamu sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'it', 'turn', 'sekarang']\tyour\n",
      "['it', 'your', 'sekarang', 'kita']\tturn\n",
      "['your', 'turn', 'kita', 'sedang']\tsekarang\n",
      "['turn', 'sekarang', 'sedang', 'belajar']\tkita\n",
      "['sekarang', 'kita', 'belajar', 'nlp']\tsedang\n",
      "['kita', 'sedang', 'nlp', '.']\tbelajar\n"
     ]
    }
   ],
   "source": [
    "# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\n",
    "for x, y in get_windows(tokenize(\"Now it's your turn: sekarang kita sedang belajar NLP!\"), 2):\n",
    "    print(f'{x}\\t{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mengubah kata menjadi vektor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "word2Ind, Ind2word = get_dict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print 'word2Ind' dictionary\n",
    "word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'i':   3\n"
     ]
    }
   ],
   "source": [
    "# Print value for the key 'i' within word2Ind dictionary\n",
    "print(\"Index of the word 'i':  \",word2Ind['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print 'Ind2word' dictionary\n",
    "Ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word which has index 2:   happy\n"
     ]
    }
   ],
   "source": [
    "# Print value for the key '2' within Ind2word dictionary\n",
    "print(\"Word which has index 2:  \",Ind2word[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5\n"
     ]
    }
   ],
   "source": [
    "V = len(word2Ind) # Save length of word2Ind dictionary into the 'V' variable\n",
    "print(\"Size of vocabulary: \", V) # Print length of word2Ind dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vektor one-hot\n",
    "\n",
    "\n",
    "cobalah dapatkan indeks dari kata \"happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = word2Ind['happy'] # Save index of word 'happy' into the 'n' variable\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buatlah vektor berukuran V dengan nilai 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_word_vector = np.zeros(V) # Create vector with the same length as the vocabulary, filled with zeros\n",
    "center_word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cek ukurannya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(center_word_vector) == V # Assert that the length of the vector is the same as the size of the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ganti elemen ke-$n$ dengan nilai 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_word_vector[n] = 1 # Replace element number 'n' with a 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inilah vektor one-hot nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "\n",
    "Buatlah fungsi berisi operasi diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    # BEGIN your code here\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    # END your code here\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_one_hot_vector('happy', word2Ind, V) # Print output of 'word_to_one_hot_vector' function for word 'happy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "\n",
    "Carilah 1-hot vektor dari kata \"learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "word_to_one_hot_vector('learning', word2Ind, V) # Print output of 'word_to_one_hot_vector' function for word 'learning'\n",
    "# END your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([0., 0., 0., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mendapatkan vektor dari context word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hitung rata-rata vektor one-hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_words = ['i', 'am', 'because', 'i'] # Define list containing context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 1., 0.]),\n",
       " array([1., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0.])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create one-hot vectors for each context word using list comprehension\n",
    "context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(context_words_vectors, axis=0) # Compute mean of the vectors using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "\n",
    "Buatlah fungsi `context_words_to_vector` berisi semua operasi diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'context_words_to_vector' function that will include the steps previously seen\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    # BEGIN your code here\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0) # Compute mean of the vectors using numpy\n",
    "    # END your code here\n",
    "    return context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\n",
    "context_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "Apakah representasi vektor dari context words \"am happy i am\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.  , 0.25, 0.25, 0.  ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "context_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V) \n",
    "# END your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([0.5 , 0.  , 0.25, 0.25, 0.  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gabungkan seluruh fungsi diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word:  happy -> [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word:  because -> [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\n",
      "Center word:  i -> [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print vectors associated to center and context words for corpus\n",
    "for context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n",
    "    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}')\n",
    "    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator function 'get_training_example'\n",
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n",
      "Center word vector:  [0. 0. 1. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n",
      "Center word vector:  [0. 1. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.25 0.25 0.25 0.   0.25]\n",
      "Center word vector:  [0. 0. 0. 1. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print vectors associated to center and context words for corpus using the generator function\n",
    "for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n",
    "    print(f'Context words vector:  {context_words_vector}')\n",
    "    print(f'Center word vector:  {center_word_vector}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The continuous bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagian ini terdiri dari:\n",
    "- Fungsi aktivasi\n",
    "- Forward propagation.\n",
    "- Cross-entropy loss.\n",
    "- Backpropagation.\n",
    "- Gradient descent.\n",
    "- Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n",
    " \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.71320643],\n",
       "       [-4.79248051],\n",
       "       [ 1.33648235],\n",
       "       [ 2.48803883],\n",
       "       [-0.01492988]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10) # Define a random seed so all random outcomes can be reproduced\n",
    "z_1 = 10*np.random.rand(5, 1)-5 # Define a 5X1 column vector using numpy\n",
    "z_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU berarti semua nilai negatif menjadi nol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = z_1.copy() # Create copy of vector and save it in the 'h' variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h < 0 # Determine which values met the criteria (this is possible because of vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[h < 0] = 0 # Slice the array or vector. This is the same as applying ReLU to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.71320643],\n",
       "       [0.        ],\n",
       "       [1.33648235],\n",
       "       [2.48803883],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h # Print the vector after ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "Buatlah fungsi ReLU dengan operasi diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'relu' function that will include the steps previously seen\n",
    "def relu(z):\n",
    "    # BEGIN your code here\n",
    "    result = z.copy() # Create copy of vector and save it in the 'h' variable\n",
    "    result[result < 0] = 0 # Slice the array or vector. This is the same as applying ReLU to it\n",
    "    # END your code here\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.50714306],\n",
       "       [2.31993942],\n",
       "       [0.98658484],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new vector and save it in the 'z' variable\n",
    "z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n",
    "relu(z) # Apply ReLU to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([[0.        ],\n",
    "           [4.50714306],\n",
    "           [2.31993942],\n",
    "           [0.98658484],\n",
    "           [0.        ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9. ,  8. , 11. , 10. ,  8.5])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([9, 8, 11, 10, 8.5]) # Define a new vector and save it in the 'z' variable\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n",
       "        4914.7688403 ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_z = np.exp(z) # Save exponentials of the values in a new vector\n",
    "e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97899.41826492078"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_e_z = np.sum(e_z) # Save the sum of the exponentials\n",
    "sum_e_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08276947985173956"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_z[0]/sum_e_z # Print softmax value of the first element in the original vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "Buatlah fungsi softmax melalui operasi diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'softmax' function that will include the steps previously seen\n",
    "def softmax(z):\n",
    "    # BEGIN your code here\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z\n",
    "    # END your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08276948 0.03044919 0.61158833 0.22499077 0.05020223]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(softmax([9, 8, 11, 10, 8.5])) # Print softmax values for original vector\n",
    "np.sum(softmax([9, 8, 11, 10, 8.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensi Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_array = np.zeros(V) # Assert that the sum of the softmax values is equal to 1\n",
    "x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_array.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_column_vector = x_array.copy() # Copy vector\n",
    "x_column_vector.shape = (V, 1)  # Reshape copy of vector # alternatively ... = (x_array.shape[0], 1)\n",
    "x_column_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_column_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3 # Define the size of the word embedding vectors and save it in the variable 'N'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inisiasi bobot dan bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first matrix of weights\n",
    "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
    "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
    "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
    "\n",
    "# Define second matrix of weights\n",
    "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
    "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
    "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
    "               [ 0.07055222, -0.02015138,  0.36107434],\n",
    "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
    "\n",
    "# Define first vector of biases\n",
    "b1 = np.array([[ 0.09688219],\n",
    "               [ 0.29239497],\n",
    "               [-0.27364426]])\n",
    "\n",
    "# Define second vector of biases\n",
    "b2 = np.array([[ 0.0352008 ],\n",
    "               [-0.36393384],\n",
    "               [-0.12775555],\n",
    "               [-0.34802326],\n",
    "               [-0.07017815]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of W1: (3, 5) (NxV)\n",
      "size of b1: (3, 1) (Nx1)\n",
      "size of W2: (3, 5) (VxN)\n",
      "size of b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "print(f'V (vocabulary size): {V}') \n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of W1: {W1.shape} (NxV)')\n",
    "print(f'size of b1: {b1.shape} (Nx1)')\n",
    "print(f'size of W2: {W1.shape} (VxN)')\n",
    "print(f'size of b2: {b2.shape} (Vx1)')\n",
    "# END your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contoh Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = get_training_example(words, 2, word2Ind, V) # Save generator object in the 'training_examples' variable with the desired arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array, y_array = next(training_examples) # Get first values from generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ubahlah vektor menjadi matriks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[0.25]\n",
      " [0.25]\n",
      " [0.  ]\n",
      " [0.5 ]\n",
      " [0.  ]]\n",
      "\n",
      "y\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "x = x_array.copy() # Copy vector\n",
    "x.shape = (V, 1) # Reshape it\n",
    "print('x')\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "y = y_array.copy()\n",
    "y.shape = (V, 1)\n",
    "print('y')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nilai dari hidden layer\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n",
    " \\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Pertama hitung nilai $\\mathbf{z_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.dot(W1, x) + b1 # Compute z1 (values of first hidden layer before applying the ReLU function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36483875],\n",
       "       [ 0.63710329],\n",
       "       [-0.3236647 ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hitung ReLU dari $\\mathbf{z_1}$ untuk memperoleh $\\mathbf{h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36483875],\n",
       "       [0.63710329],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = relu(z1) # Compute h (z1 after applying ReLU function)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nilai dari output layer\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n",
    " \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n",
    "\\end{align}\n",
    "\n",
    "**Pertama, hitung $\\mathbf{z_2}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31973737],\n",
       "       [-0.28125477],\n",
       "       [-0.09838369],\n",
       "       [-0.33512159],\n",
       "       [-0.19919612]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = np.dot(W2, h) + b2 # Compute z2 (values of the output layer before applying the softmax function)\n",
    "z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([[-0.31973737],\n",
    "           [-0.28125477],\n",
    "           [-0.09838369],\n",
    "           [-0.33512159],\n",
    "           [-0.19919612]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hitung $\\mathbf{\\hat y}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18519074],\n",
       "       [0.19245626],\n",
       "       [0.23107446],\n",
       "       [0.18236353],\n",
       "       [0.20891502]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(z2) # Compute y_hat (z2 after applying softmax function)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([[0.18519074],\n",
    "           [0.19245626],\n",
    "           [0.23107446],\n",
    "           [0.18236353],\n",
    "           [0.20891502]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cross-entropy loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18519074],\n",
       "       [0.19245626],\n",
       "       [0.23107446],\n",
       "       [0.18236353],\n",
       "       [0.20891502]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nilai target adalah:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rumus dari cross-entropy loss adalah:\n",
    "\n",
    "$$ J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_predicted, y_actual):\n",
    "    loss = np.sum(-np.log(y_predicted)*y_actual) # Fill the loss variable with your code\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4650152923611106"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(y_hat, y) # Print value of cross entropy loss for prediction and target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    1.4650152923611106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "\\begin{align}\n",
    " \\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n",
    " \\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n",
    " \\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n",
    " \\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "Hitung variabel `grad_b2` sebagai berikut\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18519074],\n",
       "       [ 0.19245626],\n",
       "       [-0.76892554],\n",
       "       [ 0.18236353],\n",
       "       [ 0.20891502]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "grad_b2 = y_hat - y # Compute vector with partial derivatives of loss function with respect to b2\n",
    "# END your code here\n",
    "\n",
    "grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([[ 0.18519074],\n",
    "           [ 0.19245626],\n",
    "           [-0.76892554],\n",
    "           [ 0.18236353],\n",
    "           [ 0.20891502]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "Hitung variabel `grad_W2` sebagai berikut\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06756476,  0.11798563,  0.        ],\n",
       "       [ 0.0702155 ,  0.12261452,  0.        ],\n",
       "       [-0.28053384, -0.48988499, -0.        ],\n",
       "       [ 0.06653328,  0.1161844 ,  0.        ],\n",
       "       [ 0.07622029,  0.13310045,  0.        ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "grad_W2 = np.dot(y_hat - y, h.T) # Compute matrix with partial derivatives of loss function with respect to W2\n",
    "# END your code here\n",
    "\n",
    "grad_W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([[ 0.06756476,  0.11798563,  0.        ],\n",
    "           [ 0.0702155 ,  0.12261452,  0.        ],\n",
    "           [-0.28053384, -0.48988499,  0.        ],\n",
    "           [ 0.06653328,  0.1161844 ,  0.        ],\n",
    "           [ 0.07622029,  0.13310045,  0.        ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "\n",
    "Hitung variabel `grad_b1` sebagai berikut\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.17045858]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "grad_b1 = relu(np.dot(W2.T, y_hat - y)) # Compute vector with partial derivatives of loss function with respect to b1\n",
    "# END your code here\n",
    "\n",
    "grad_b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([[0.        ],\n",
    "           [0.        ],\n",
    "           [0.17045858]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "\n",
    "Hitung variabel `grad_W1` sebagai berikut\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "grad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T) # Compute matrix with partial derivatives of loss function with respect to W1\n",
    "# END your code here\n",
    "\n",
    "grad_W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
    "           [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
    "           [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cek semua dimensi tensor diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (vocabulary size): 5\n",
      "N (embedding size / size of the hidden layer): 3\n",
      "size of grad_W1: (3, 5) (NxV)\n",
      "size of grad_b1: (3, 1) (Nx1)\n",
      "size of grad_W2: (3, 5) (VxN)\n",
      "size of grad_b2: (5, 1) (Vx1)\n"
     ]
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "print(f'V (vocabulary size): {V}')\n",
    "print(f'N (embedding size / size of the hidden layer): {N}')\n",
    "print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n",
    "print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n",
    "print(f'size of grad_W2: {grad_W1.shape} (VxN)')\n",
    "print(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n",
    "# END your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n",
    " \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n",
    " \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n",
    " \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01 # Define alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perbaharui bobot $\\mathbf{W_1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_new = W1 - alpha * grad_W1 # Compute updated W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandingkan nilai lama dan baru bobot $\\mathbf{W_1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n",
      "\n",
      "new value of W1:\n",
      "[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n",
      " [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n",
      " [ 0.26594987 -0.23889501 -0.37770863 -0.11484675  0.34008124]]\n"
     ]
    }
   ],
   "source": [
    "print('old value of W1:')\n",
    "print(W1)\n",
    "print()\n",
    "print('new value of W1:')\n",
    "print(W1_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas\n",
    "\n",
    "Hitung gradient descent lainnya\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n",
    " \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n",
    " \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2_new\n",
      "[[-0.22249629 -0.43126617  0.13310965]\n",
      " [ 0.08406387  0.08000579  0.1772054 ]\n",
      " [ 0.18996044 -0.05617378 -0.1790735 ]\n",
      " [ 0.06988689 -0.02131322  0.36107434]\n",
      " [ 0.33404254 -0.39556489 -0.43959196]]\n",
      "\n",
      "b1_new\n",
      "[[ 0.09688219]\n",
      " [ 0.29239497]\n",
      " [-0.27534885]]\n",
      "\n",
      "b2_new\n",
      "[[ 0.03334889]\n",
      " [-0.3658584 ]\n",
      " [-0.12006629]\n",
      " [-0.3498469 ]\n",
      " [-0.0722673 ]]\n"
     ]
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "W2_new = W2 - alpha * grad_W2 # Compute updated W2\n",
    "b1_new = b1 - alpha * grad_b1 # Compute updated b1\n",
    "b2_new = b2 - alpha * grad_b2 # Compute updated b2\n",
    "# END your code here\n",
    "\n",
    "print('W2_new')\n",
    "print(W2_new)\n",
    "print()\n",
    "print('b1_new')\n",
    "print(b1_new)\n",
    "print()\n",
    "print('b2_new')\n",
    "print(b2_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    W2_new\n",
    "    [[-0.22384758 -0.43362588  0.13310965]\n",
    "     [ 0.08265956  0.0775535   0.1772054 ]\n",
    "     [ 0.19557112 -0.04637608 -0.1790735 ]\n",
    "     [ 0.06855622 -0.02363691  0.36107434]\n",
    "     [ 0.33251813 -0.3982269  -0.43959196]]\n",
    "\n",
    "    b1_new\n",
    "    [[ 0.09688219]\n",
    "     [ 0.29239497]\n",
    "     [-0.27875802]]\n",
    "\n",
    "    b2_new\n",
    "    [[ 0.02964508]\n",
    "     [-0.36970753]\n",
    "     [-0.10468778]\n",
    "     [-0.35349417]\n",
    "     [-0.0764456 ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "\n",
    "### Optsi 1: embedding dari $\\mathbf{W_1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
       "       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
       "       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "because\n",
      "happy\n",
      "i\n",
      "learning\n"
     ]
    }
   ],
   "source": [
    "# Print corresponding word for each index within vocabulary's range\n",
    "for i in range(V):\n",
    "    print(Ind2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [0.41687358 0.32735501 0.26637602]\n",
      "because: [ 0.08854191  0.22795148 -0.23846886]\n",
      "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
      "i: [ 0.28320538  0.4117634  -0.11399446]\n",
      "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
     ]
    }
   ],
   "source": [
    "# loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W1[:, word2Ind[word]]\n",
    "    \n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opsi 2: embedding dari $\\mathbf{W_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n",
       "       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n",
       "       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [-0.22182064 -0.43008631  0.13310965]\n",
      "because: [0.08476603 0.08123194 0.1772054 ]\n",
      "happy: [ 0.1871551  -0.06107263 -0.1790735 ]\n",
      "i: [ 0.07055222 -0.02015138  0.36107434]\n",
      "learning: [ 0.33480474 -0.39423389 -0.43959196]\n"
     ]
    }
   ],
   "source": [
    "# loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W2.T[:, word2Ind[word]]\n",
    "    \n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opsi 3: embedding dari $\\mathbf{W_1}$ and $\\mathbf{W_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
       "       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
       "       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN your code here\n",
    "W3 = (W1+W2.T)/2 # Compute W3 as the average of W1 and W2 transposed\n",
    "# END your code here\n",
    "\n",
    "W3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
    "           [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
    "           [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am: [ 0.09752647 -0.05136565  0.19974284]\n",
      "because: [ 0.08665397  0.15459171 -0.03063173]\n",
      "happy: [-0.02389858 -0.15029611 -0.27839106]\n",
      "i: [0.1768788  0.19580601 0.12353994]\n",
      "learning: [ 0.3764029  -0.31673866 -0.04975536]\n"
     ]
    }
   ],
   "source": [
    "# loop through each word of the vocabulary\n",
    "for word in word2Ind:\n",
    "    # extract the column corresponding to the index of the word in the vocabulary\n",
    "    word_embedding_vector = W3[:, word2Ind[word]]\n",
    "    \n",
    "    print(f'{word}: {word_embedding_vector}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferensiasi menggunakan Autograd\n",
    "=======================================\n",
    "PyTorch mempunyai mekanisme diferensiasi bawaan yang disebut dengan ``torch.autograd``. Mari kita lihat bagaimana ``autograd`` mengumpulkan gradien. Buat 2 tensor ``a`` dan ``b`` dengan\n",
    "``requires_grad=True``. Ini memberi arahan bagi ``autograd`` bahwa setiap operasi terhadap tensor ini harus dilacak.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita membuat tensor lain ``Q`` dari ``a`` dan ``b``.\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.,  65.], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = 3*a**3 - b**2\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumsikan ``a`` dan ``b`` sebagai parameter dari NN, dan ``Q`` eror. Di proses pelatihan NN, kita ingin gradien dari eror terhadap parameter, dimana: \n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "\n",
    "Ketika kita memanggil ``.backward()`` pada ``Q``, autograd menghitung gradien ini dan. menyimpannya dalam atribut ``.grad`` dari tensor tersebut.\n",
    "\n",
    "Kita harus menggunakan argumen ``gradient`` di ``Q.backward()`` karena ini adalah sebuah vektor. ``gradient`` adalah sebuah tensor dari bentuk yang sama dengan ``Q``, dan merepresentasikan gradien dari Q terhadap dirinya sendiri, dimana:\n",
    "\n",
    "\\begin{align}\\frac{dQ}{dQ} = 1\\end{align}\n",
    "\n",
    "Kita dapat pula agregasi Q menjadi sebuah skalar dan memanggil backward secara implisit, seperti ``Q.sum().backward()``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradien sekarang telah disimpan dalam ``a.grad`` dan ``b.grad``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# cek apakah gradien yang tersimpan terhitung benar\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soal\n",
    "----------\n",
    "Buatlah loss function sebagai berikut:\n",
    "\\begin{align}Y = \\Sigma  ln(x) \\end{align}\n",
    "\n",
    "Gunakan autograd untuk menghitung gradiennya terhadap parameter a\n",
    "\n",
    "Hitung turunannya secara analitik dan bandingkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.3333])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2., 3.], requires_grad=True)\n",
    "Y = torch.sum(torch.log(x))\n",
    "# backward pass\n",
    "Y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferensiasi Neural Network Otomatis melalui ``torch.autograd``\n",
    "=======================================\n",
    "\n",
    "Algoritma untuk melatih neural networks adalah **back propagation**. Pada algoritma ini, parameters (bobot) akan diubah sesuai dengan **gradient** dari loss function terhadap parameter tersebut.\n",
    "\n",
    "Untuk menghitung gradient itu, PyTorch mempunyai mekanisme diferensiasi bawaan yang disebut dengan ``torch.autograd``. Ini mempermudah perhitungan gradien secara otomatis terhadap computational graph jenis apapun.\n",
    "\n",
    "Cobalah 1-layer neural network, dengan input  ``x``,parameters ``w`` and ``b``, dan loss function sebagai berikut:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1214, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soal\n",
    "-----\n",
    "\n",
    "Hitung binary cross entropy alternatif dengan menggunakan operasi tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1214, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def BCE(z, y):\n",
    "    p = -y*(torch.log(torch.sigmoid(z)))-(1-y)*torch.log(1-torch.sigmoid(z))\n",
    "    return p.mean()\n",
    "\n",
    "loss_manual = BCE(z,y)\n",
    "loss_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor dan Gradien\n",
    "------------------------------------------\n",
    "\n",
    "pada network ini, ``w`` dan ``b`` adalah **parameters**, yang akan kita optimalisasi. \n",
    "Maka, kita memerlukan perhitungan gradien dari sebuah loss function terhadap variabel-variabel tersebut. Untuk bisa melakukannya, kita mengatur ``requires_grad`` dari tensor-tensor tersebut.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Catatan</h4><p>Kamu dapat mengatur ``requires_grad`` ketika menciptakan sebuah tensor, atau mengaturnya nanti dengan menggunakan metode ``x.requires_grad_(True)``.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebuah fungsi yang kita aplikasikan ke tensor sebagai kontruksi computational graph adalah sebuah kelas objek ``Function``. Objek ini mengerti bagaimana cara menghitung fungsi dalam arah *forward*, dan juga bagaimana cara menghitung turunannya selama langkah-langkah *backward propagation*. Sebuah referensi terhadap fungsi backward propagation disimpan dalam properti ``grad_fn`` dari sebuah tensor. Kamu dapat menemukan informasi lebih lanjut mengenai ``Function`` dalam dokumentasi berikut\n",
    "https://pytorch.org/docs/stable/autograd.html#function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000001F00B5012D0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001F002B7F280>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghitung Gradien\n",
    "-------------------\n",
    "\n",
    "Untuk optimalisasi bobot pada neural network, kita perlu menghitung turunan dari loss function terhadap parameter. Secara khusus, kita memerlukan $\\frac{\\partial loss}{\\partial w}$ dan\n",
    "$\\frac{\\partial loss}{\\partial b}$ terhadap nilai dari ``x`` and ``y``. Untuk menghitung turunan itu, kita memanggil ``loss.backward()``, lalu mendapatkan nilai ``w.grad`` dan\n",
    "``b.grad``:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2523, 0.2584, 0.1223],\n",
      "        [0.2523, 0.2584, 0.1223],\n",
      "        [0.2523, 0.2584, 0.1223],\n",
      "        [0.2523, 0.2584, 0.1223],\n",
      "        [0.2523, 0.2584, 0.1223]])\n",
      "tensor([0.2523, 0.2584, 0.1223])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Catatan</h4><p>- Kita akan memperoleh properti ``grad`` untuk leaf node dari computational graph, dimana properti ``requires_grad`` diatur menjadi ``True``. Untuk semua nodes yang lain di graph ini, gradien tidak tersedia. \n",
    "    - Kita hanya dapat melakukan penghitungan gradien menggunakan ``backward`` sekali pada graph tersebut, untuk alasan performa. Jika kita membutuhkan memanggil beberapa ``backward`` pada graph yang sama, kita perlu melakukan ``retain_graph=True`` to the ``backward`` call.</p></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mematikan Pelacakan Gradien\n",
    "---------------------------\n",
    "\n",
    "Secara bawaan, semua tensor dengan ``requires_grad=True`` akan melacak sejarah komputasi dan mendukung komputasi gradien. Tetapi, terdapat beberapa kasus dimana kita tidak perlu melakukannya. Misalnya ketika kita melatih model dan hanya ingin menggunakannya terhadap beberapa input data, kita hanya perlu melakukan komputasi *forward*. Kita bisa berhenti melacak komputasi dengan menggunakan blok ``torch.no_grad()``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cara lain untuk memperoleh hasil yang sama adalah dengan menggunakan metode ``detach()`` pada tensor:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada alasan lain untuk mematikan pelacakan gradien:\n",
    "    - Untuk menandai beberapa parameter di neural network sebagai **frozen parameters**. Ini skenario yang sangat umum untuk `finetuning a pretrained network` https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "    - Untuk mempercepat perhitungan ketika hanya menghitung gerakan forward, karena komputasi pada tensors tanpa pelacakan gradien akan jauh lebih efisien\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambahan untuk Computational Graphs\n",
    "----------------------------\n",
    "Autograd menyimpan catatan data (tensors) dan semua operasi yanbg tereksekusi (bersama dengan hasil tensor yang baru) di sebuah directed acyclic graph (DAG) yang berisi objek `Function` https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function. Pada DAG ini, leaves adalah input tensor, root adalah output tensor. Dengan melacak jejaring ini dari roots ke leaves, kamu dapat secara otomatis menghitung gradien dengan menggunakan aturan rantai.\n",
    "\n",
    "Di sebuah gerakan forward, autograd melakukan 2 hal sekaligus:\n",
    "- menjalankan operasi untuk menghitung hasil tensor\n",
    "- menjaga operasi *gradient function* pada DAG\n",
    "\n",
    "Gerakan backward dimulai ketika ``.backward()`` dipanggil pada DAG root. \n",
    "``autograd`` kemudian:\n",
    "- menghitung gradien dari setiap ``.grad_fn``\n",
    "- mengakumulasi semuanya pada tensor dengan atribut ``.grad``\n",
    "- menggunakan aturan rantai, melakukan propagasi semuanya sampai kepada tensor leaf.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Catatan</h4><p>**DAGs bersifat dinamis di PyTorch**\n",
    "    Sebuah hal penting untuk diperhatikan adalah jejaring yang diciptakan dari kosong; setelah masing-masing pemanggilan ``.backward()``, autograd mulai mengisi jejaring yang baru. Ini yang mengijinkanmu menggunakan control flow statements pada modelmu; kamu dapat merubah bentuk, ukuran, dan operasi pada setiap iterasi jika diperlukan\n",
    "</p></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soal\n",
    "-----\n",
    "\n",
    "\n",
    "Gunakan pretrained resnet18 dari torchvision. Kita akan membuat tensor berisi random data sebagai representasi dari 1 gambar dengan 3 chanel, tinggi & lebarnya adalah 64, dan labelnya diinisiasi dengan secara random. Labelnya berbentuk (1,1000). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleve\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\cleve\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan prediksi label berdasarkan model dan data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.7145e-01, -5.4468e-01, -7.8834e-01, -1.6376e+00, -8.3372e-01,\n",
       "         -2.3858e-01, -5.6237e-01,  7.1084e-01,  3.9650e-01, -5.6695e-01,\n",
       "         -9.0019e-01, -6.0880e-01,  5.7543e-02, -7.2010e-01, -7.3397e-01,\n",
       "         -5.8410e-01, -6.0299e-01,  9.8909e-02, -4.1788e-01, -4.7234e-01,\n",
       "         -1.5700e+00, -6.7385e-01, -1.6543e+00,  1.8175e-01, -1.0010e+00,\n",
       "         -1.3930e+00, -9.8390e-01, -1.4459e+00, -8.8328e-01, -2.4135e-01,\n",
       "         -5.4351e-01, -8.2394e-01, -3.2191e-01, -5.3382e-01, -4.1670e-01,\n",
       "         -2.6885e-01,  9.0149e-01, -5.2071e-01, -2.4656e-01,  2.6988e-01,\n",
       "         -6.7325e-01, -6.8615e-01, -9.2778e-01, -2.4396e-01, -3.7374e-01,\n",
       "         -3.9633e-01, -5.6786e-01, -4.9382e-01, -1.1448e+00, -8.8988e-01,\n",
       "         -3.4014e-01,  3.6016e-01, -2.9857e-01, -4.4654e-01, -5.7892e-02,\n",
       "         -1.0531e+00, -4.0917e-01, -1.3842e+00, -3.5472e-01, -9.9625e-02,\n",
       "          9.6480e-01,  9.2751e-02, -1.5854e-01,  3.6682e-01, -5.0781e-01,\n",
       "         -2.7655e-01, -1.2332e-01, -1.6674e-01, -6.3864e-01, -1.0680e+00,\n",
       "         -1.5806e+00,  5.1752e-02, -1.1814e+00, -3.6317e-02, -1.0802e+00,\n",
       "         -1.1703e+00,  2.0432e-01, -3.1323e-01,  2.8670e-01,  2.9302e-01,\n",
       "         -7.9819e-01, -1.6855e+00, -5.0524e-02, -7.5173e-01, -2.1614e-01,\n",
       "          2.9643e-02,  2.5430e-01,  3.8937e-01,  4.7774e-01, -7.7990e-01,\n",
       "         -9.9262e-01, -9.5694e-01, -1.5841e+00, -1.9628e-01,  1.8634e-01,\n",
       "         -1.9151e+00, -4.8176e-01, -6.0051e-01, -1.4968e+00, -5.3224e-01,\n",
       "         -1.6016e+00, -1.0825e+00, -1.1907e+00, -4.3532e-01, -3.0972e-01,\n",
       "         -5.6493e-01, -6.4380e-01, -1.4581e+00, -1.0520e+00, -1.6092e+00,\n",
       "         -1.2920e+00, -6.1064e-01,  1.0294e+00,  5.9793e-01,  4.6310e-01,\n",
       "         -1.0939e+00, -1.1738e+00, -1.3900e-01,  5.6831e-01, -4.5160e-01,\n",
       "         -1.0179e+00, -1.1361e-01,  1.6041e-01,  6.0444e-02,  1.0319e+00,\n",
       "          4.9957e-02,  3.2263e-01, -1.2792e+00, -1.0979e+00, -1.0519e+00,\n",
       "         -1.4673e+00, -1.2293e+00, -8.3909e-01, -1.2870e+00, -5.5192e-01,\n",
       "         -1.1634e+00, -7.7224e-01, -1.2461e+00, -1.1162e+00, -1.7020e+00,\n",
       "         -1.6062e+00, -1.6889e+00, -2.2861e+00, -1.7618e+00, -7.9680e-01,\n",
       "         -7.4729e-01, -1.3125e+00, -2.2551e+00, -1.4793e+00, -1.4718e+00,\n",
       "          9.1918e-02,  1.5253e+00, -1.0127e+00, -3.3103e-01,  1.5852e-01,\n",
       "          3.1652e-02, -3.8410e-01, -2.7008e-01,  2.3136e-02, -9.3281e-03,\n",
       "          1.8130e-01,  6.9599e-01,  2.6776e-02,  5.2860e-01,  1.6521e-01,\n",
       "         -3.2664e-01, -4.0458e-01, -8.9094e-01,  5.0789e-01, -6.9890e-01,\n",
       "         -3.7990e-02,  9.0749e-01,  3.1605e-01,  2.2397e-01, -2.8494e-01,\n",
       "         -1.0829e+00, -1.5099e-01,  5.6817e-03,  5.6877e-01,  4.0628e-01,\n",
       "          3.9810e-01, -5.5252e-01,  2.9521e-01, -1.2345e-01,  3.9018e-01,\n",
       "          5.0066e-01,  2.2929e-01,  1.9224e-01, -2.3687e-01,  1.6802e-01,\n",
       "         -6.5489e-01,  3.5920e-02,  1.7185e-01,  3.8440e-01, -9.3660e-01,\n",
       "          7.1508e-01, -1.3366e-01, -4.1474e-02,  7.7815e-02,  4.8289e-01,\n",
       "         -1.4201e-01,  5.3133e-02,  4.3706e-01,  2.7932e-01, -3.2021e-02,\n",
       "          4.8068e-02, -4.1976e-01,  4.1793e-01,  1.2301e+00,  1.9659e-01,\n",
       "         -1.5170e-01,  2.8761e-01,  2.1496e-01, -1.2894e-01,  4.1647e-02,\n",
       "          2.3576e-01, -2.5112e-01,  7.2782e-02, -5.3249e-01,  5.5825e-01,\n",
       "         -6.7530e-02, -4.6262e-01, -3.6791e-01,  3.7936e-01,  1.2813e-01,\n",
       "          2.3081e-01, -1.8422e-01,  6.1584e-01, -6.4136e-01, -4.1914e-01,\n",
       "         -1.2063e-01,  1.6980e-01,  1.7901e-01, -4.3451e-01,  6.2064e-01,\n",
       "          3.0134e-01,  3.4370e-01,  3.4487e-01,  6.3444e-01, -1.7694e-01,\n",
       "          3.3162e-01, -2.2922e-01,  4.9054e-01,  4.1370e-01, -5.0454e-01,\n",
       "          2.6939e-01,  3.7573e-01, -1.3337e-01,  2.2535e-01, -1.9812e-01,\n",
       "          7.1830e-02,  4.7677e-01, -9.1928e-01,  4.3261e-01,  1.0409e+00,\n",
       "         -9.1312e-01,  1.4754e-01,  1.1396e-01, -4.3806e-01,  1.0824e-01,\n",
       "         -4.9456e-01, -9.9246e-01, -2.7519e-01,  1.1116e-01,  2.4163e-01,\n",
       "          5.2514e-01, -1.8804e-02,  1.3699e-01, -1.8857e-01, -7.1332e-01,\n",
       "         -1.1300e+00, -1.2821e+00, -8.1804e-01,  3.2538e-01, -1.3345e+00,\n",
       "         -1.2961e+00, -1.3613e+00, -8.3614e-01, -1.4257e+00, -1.0056e+00,\n",
       "         -4.9270e-01,  9.3781e-01,  8.9661e-01,  1.9379e-01,  5.8603e-01,\n",
       "          9.9791e-01, -2.8172e-01, -1.8333e-01, -6.7958e-01, -1.7415e+00,\n",
       "         -9.2544e-01, -1.5063e+00, -3.1026e-01, -1.2075e+00, -1.3324e+00,\n",
       "         -9.5270e-01, -1.2837e+00, -1.5114e+00, -6.1079e-01, -3.8654e-01,\n",
       "         -1.9114e+00, -6.7626e-01, -5.2743e-01, -3.0893e-01, -1.0692e+00,\n",
       "         -7.1668e-01,  3.2446e-01, -8.2857e-01, -9.7280e-01, -3.1148e-01,\n",
       "          5.1647e-01, -1.0837e-01, -3.6701e-01,  2.9818e-01,  7.0794e-01,\n",
       "         -4.0965e-01, -5.2436e-01, -7.7388e-01, -1.0753e+00, -1.4767e-01,\n",
       "         -1.2906e+00, -1.1712e+00, -1.2017e+00, -1.4744e+00, -1.3529e+00,\n",
       "         -1.7183e+00, -9.7627e-01, -1.3227e-02, -3.5726e-01, -7.9004e-01,\n",
       "         -1.2086e-01, -4.1011e-01,  1.0821e-01,  6.8984e-02, -7.5464e-01,\n",
       "         -8.0755e-01, -1.7224e+00,  2.2139e-03,  7.5407e-01, -1.1314e+00,\n",
       "         -5.5451e-01,  3.6952e-01, -6.2270e-01, -1.4819e+00, -9.3467e-01,\n",
       "          7.4942e-01, -5.3701e-01, -1.7363e+00, -1.2715e-01, -1.3514e+00,\n",
       "         -1.4055e+00, -2.1389e+00, -1.1872e+00, -7.2300e-01, -7.2551e-01,\n",
       "          3.1973e-01,  7.5693e-01, -1.6104e-01,  3.2044e-01,  2.9832e-01,\n",
       "         -5.2565e-01,  4.4094e-01, -2.0476e-01,  1.4524e-01, -6.4627e-01,\n",
       "         -5.3625e-01, -8.9676e-01, -1.2461e-01, -9.6058e-01, -4.7595e-01,\n",
       "         -5.3283e-01, -2.4838e-01, -5.4525e-01,  2.1752e-01, -1.5643e-01,\n",
       "         -7.6858e-01, -6.7485e-01,  3.1227e-01, -2.0843e-01, -4.4108e-01,\n",
       "          4.1170e-01, -3.8722e-01, -3.0058e-01, -7.7457e-01, -8.8871e-01,\n",
       "         -5.6461e-01, -1.0370e+00, -1.3187e+00, -1.2087e+00, -3.8738e-01,\n",
       "          6.8627e-01, -1.7056e-01, -1.3560e+00, -1.7255e+00, -2.1223e-01,\n",
       "          2.7735e-01, -1.1979e+00, -6.7212e-01,  3.6560e-01,  3.6045e-01,\n",
       "         -5.8284e-01,  8.4877e-01,  3.2474e-01, -2.2211e+00, -1.9670e+00,\n",
       "         -8.5253e-01, -1.0492e-01, -3.4469e-01, -2.9133e-01,  8.9355e-01,\n",
       "         -2.0115e-02,  2.8805e-01,  2.3243e+00,  6.8180e-01,  5.1032e-01,\n",
       "          1.0488e+00, -3.2531e-01,  1.4735e-01,  3.1523e-01,  1.0697e+00,\n",
       "          7.8736e-01,  1.4464e+00,  2.2083e-02,  1.8912e-01,  5.9436e-01,\n",
       "         -8.3040e-01, -1.1446e-02,  1.3728e+00,  1.6163e+00,  4.8170e-01,\n",
       "         -6.2151e-01, -1.0614e-01,  3.3719e-01,  8.1534e-01,  4.4189e-01,\n",
       "          1.0457e+00, -3.0248e-01, -6.8926e-01,  5.9309e-01,  5.5608e-01,\n",
       "          7.4522e-01,  5.2586e-01,  2.2937e-01, -3.9699e-01, -1.7776e-01,\n",
       "          2.0659e-01,  6.7832e-01,  1.3165e+00,  1.0350e+00, -6.3325e-01,\n",
       "         -1.2690e-01,  3.1603e-01,  5.8501e-01,  1.7884e-02, -1.5530e-01,\n",
       "          7.0289e-01,  1.4980e+00,  1.3287e+00,  3.1284e-03,  6.6739e-01,\n",
       "         -7.6911e-01,  6.9058e-01,  1.3274e+00,  2.3399e+00,  9.4988e-01,\n",
       "         -3.3861e-01, -1.0077e+00, -2.3149e-01, -7.4892e-02,  1.4754e+00,\n",
       "          1.1311e+00,  3.3436e-01, -5.4932e-02,  1.0486e+00, -1.3021e-01,\n",
       "         -1.7712e-01,  3.2631e-01,  5.3486e-01,  1.1557e+00,  4.2517e-01,\n",
       "          1.7587e-01,  3.2630e-01,  3.3516e-01, -1.2389e+00, -1.3007e+00,\n",
       "          2.0305e-01, -4.2471e-01,  1.3206e+00,  1.5611e+00,  1.0655e+00,\n",
       "          2.5090e-01,  7.6768e-01,  7.6480e-01, -9.6623e-01,  1.4282e+00,\n",
       "         -6.3297e-01,  8.2455e-02, -4.3042e-01, -9.4966e-02,  1.3075e+00,\n",
       "         -1.9352e+00,  5.0390e-01,  1.4218e+00,  7.1467e-01,  1.2008e+00,\n",
       "          1.4564e+00,  1.2169e+00,  7.6175e-01,  5.5888e-01,  5.1274e-01,\n",
       "         -1.4510e+00, -1.2846e+00,  1.1046e+00,  4.6515e-01,  1.0848e+00,\n",
       "          1.7723e+00,  2.3808e-01,  7.3492e-02,  1.3998e+00,  1.0753e+00,\n",
       "         -6.1788e-01,  4.2385e-01,  9.4933e-01,  1.8119e+00,  3.7219e-01,\n",
       "         -6.9932e-01,  3.8471e-01, -3.5403e-01,  7.3152e-01, -5.2410e-02,\n",
       "          9.1570e-01,  4.0990e-02,  3.6633e-01, -1.1068e+00,  6.7038e-01,\n",
       "         -5.8883e-01, -4.6506e-01, -8.3414e-01,  2.1553e-01,  1.5684e+00,\n",
       "         -1.3521e+00,  1.7338e+00,  1.1919e+00,  7.9027e-01,  4.8003e-01,\n",
       "          1.2192e+00,  7.8848e-01, -2.0224e+00, -1.0457e+00,  1.0479e-01,\n",
       "         -1.9509e-01,  2.3534e-02,  1.0779e+00,  1.4690e-01, -1.7763e+00,\n",
       "         -7.6600e-01,  2.9391e-01,  3.0616e-01,  1.1714e+00,  9.5325e-01,\n",
       "          1.7195e-01, -7.4475e-03,  1.0976e+00,  2.8032e-01, -1.0997e+00,\n",
       "         -6.4354e-01,  2.4243e-01,  7.8038e-01,  4.0420e-01, -5.2159e-01,\n",
       "          1.1573e+00, -1.5542e-02,  9.2296e-01, -8.7979e-01,  4.7723e-01,\n",
       "         -3.9222e-01, -1.3342e+00,  1.2717e+00,  1.0060e-01,  3.6118e-01,\n",
       "          4.2595e-01, -1.2142e-01,  7.9398e-01,  1.1269e+00,  8.4904e-01,\n",
       "          8.4739e-01, -2.2656e-01,  1.8910e+00,  1.3243e+00,  1.2546e+00,\n",
       "         -6.1103e-01,  5.0538e-01, -1.4604e-01,  8.9385e-01,  3.0457e-01,\n",
       "         -6.2069e-01,  1.0950e+00, -2.3593e-01, -5.7227e-01,  8.7186e-01,\n",
       "          2.1149e+00, -2.1972e-01, -2.0232e-01, -4.9304e-01,  7.1494e-01,\n",
       "          2.4940e-01,  1.4643e+00, -6.1905e-01,  7.0790e-01, -4.1335e-01,\n",
       "          1.0607e+00,  1.0224e+00, -5.0614e-01,  6.1334e-01,  2.6461e-01,\n",
       "          3.5510e-01,  1.1581e+00,  6.3673e-01,  1.9443e+00,  7.8250e-01,\n",
       "          1.4237e+00,  7.1540e-01,  1.9621e-01,  4.7992e-01,  3.1207e-01,\n",
       "         -1.4925e+00,  1.1027e+00, -3.2925e-01, -1.3247e+00,  3.3847e-01,\n",
       "          1.5514e-01,  9.2712e-01,  8.4621e-01,  1.3569e+00, -1.5159e-01,\n",
       "          3.5669e-01,  1.1724e+00,  9.2365e-01,  7.3444e-01,  1.1119e-01,\n",
       "         -1.6483e+00,  9.5719e-01,  4.4550e-01,  1.5678e+00,  8.8923e-01,\n",
       "         -1.0288e+00,  8.5882e-01,  7.1803e-01, -5.5027e-01, -2.0059e+00,\n",
       "          8.0675e-01,  3.2341e-01,  9.6134e-01,  1.0839e+00, -4.4511e-01,\n",
       "          7.4772e-01, -1.9237e-01, -8.7358e-02,  3.4805e-02,  4.4303e-01,\n",
       "          5.5680e-02, -1.0005e+00,  7.8893e-02, -8.8065e-01,  1.0157e+00,\n",
       "          8.2612e-02,  1.1519e+00,  7.7512e-01, -6.5662e-01, -1.9457e-01,\n",
       "          3.2400e-01,  6.8934e-02, -4.6758e-01,  9.3021e-01,  1.8222e+00,\n",
       "         -6.9773e-01,  1.6031e+00,  1.1885e+00,  9.2832e-01,  2.5578e-01,\n",
       "          5.6713e-01,  7.6711e-01, -6.3258e-01,  4.5239e-01,  7.4396e-01,\n",
       "         -1.6978e+00,  3.2081e-02, -1.1238e+00, -5.8235e-02, -9.7028e-01,\n",
       "         -5.4954e-01,  9.4933e-01,  9.2095e-01,  2.8456e-01, -1.1882e+00,\n",
       "          1.0535e+00,  1.7718e+00,  3.4392e-01, -2.7405e-01,  7.2945e-01,\n",
       "          1.9335e+00, -3.9920e-01, -1.7029e-01,  3.2837e-01,  8.3365e-01,\n",
       "         -4.9397e-01, -7.2205e-02,  5.8805e-01,  1.2179e+00,  2.1723e-01,\n",
       "          1.1526e+00,  1.0239e+00,  1.3517e-01, -1.6062e-01,  4.5388e-01,\n",
       "         -2.7695e-01,  6.5554e-01, -8.8637e-01, -3.6264e-01,  9.6523e-01,\n",
       "          2.5206e-01,  4.1472e-01,  1.5192e+00,  4.6888e-01, -7.5060e-01,\n",
       "          1.3167e+00, -6.2022e-01, -7.3844e-02,  1.5159e+00, -2.6249e-01,\n",
       "          1.6398e-01,  2.2437e+00, -5.6701e-01,  1.7549e+00, -1.4580e+00,\n",
       "          3.4528e-01,  1.5675e-01,  7.9882e-01,  1.2163e+00,  5.8584e-01,\n",
       "          1.1966e+00,  1.8592e-01,  6.0164e-01,  5.1219e-01,  5.1437e-01,\n",
       "          2.6305e-01,  1.9672e-01,  6.0794e-01,  6.8778e-01,  1.9449e+00,\n",
       "          6.0500e-01, -2.1872e-01,  1.7877e-01,  9.2003e-01,  9.4017e-01,\n",
       "         -5.7313e-01,  1.4178e+00, -2.2737e-01,  1.1418e+00, -1.8110e-01,\n",
       "          2.1005e-01,  9.2320e-01,  5.1801e-01,  8.1099e-01,  1.2312e+00,\n",
       "          9.9873e-01,  2.3024e-01,  5.9363e-01, -3.6197e-01,  1.6790e+00,\n",
       "          7.0234e-01,  7.6581e-01,  1.2602e+00,  8.8768e-01,  1.0083e+00,\n",
       "          4.6781e-01,  5.2705e-01,  4.1862e-01,  1.5309e+00, -8.5821e-01,\n",
       "         -1.1433e+00, -7.1049e-01,  9.6365e-01,  9.8815e-01,  1.5172e+00,\n",
       "          1.2132e-01,  6.5178e-01,  1.0173e+00,  4.9034e-01,  2.6655e-01,\n",
       "          8.2421e-01,  1.2626e+00,  1.5848e+00,  1.1302e+00,  5.0957e-01,\n",
       "          3.9203e-01,  1.0891e+00,  6.8361e-01, -7.1279e-01,  6.8527e-01,\n",
       "         -7.8427e-01,  1.8637e-01, -9.8517e-01, -1.2392e+00,  1.3085e+00,\n",
       "          1.2256e+00,  2.9384e-01,  4.5403e-01,  1.4793e+00,  6.4246e-02,\n",
       "         -6.7051e-01,  1.2941e+00, -4.7176e-01,  1.5266e+00, -1.1416e+00,\n",
       "         -2.5698e-01,  3.7893e-01, -1.2861e+00,  1.6185e+00,  4.4464e-01,\n",
       "         -1.6117e+00, -1.0952e+00,  8.7376e-01,  7.5830e-01,  1.0083e+00,\n",
       "         -1.1455e+00,  3.1590e-01,  1.0297e+00,  1.1610e+00, -4.5392e-01,\n",
       "          1.1532e+00,  5.3544e-01, -9.4308e-01, -1.1540e+00,  2.8917e-01,\n",
       "          7.8427e-01,  1.5970e+00,  1.5129e+00,  8.8115e-01, -1.0203e+00,\n",
       "          1.7409e+00,  6.0608e-01,  3.5585e-01,  3.8939e-01,  5.8367e-01,\n",
       "          1.6668e+00,  9.1084e-01, -5.1841e-01,  4.6848e-01,  1.1588e+00,\n",
       "          1.3644e+00,  1.3541e+00,  1.7384e+00, -4.6929e-01, -2.5670e-01,\n",
       "          9.1296e-01, -7.9948e-01, -1.1749e-01, -2.7037e-01,  1.3619e+00,\n",
       "          4.8757e-01,  1.4673e+00,  1.1104e+00, -6.9136e-02, -4.5073e-01,\n",
       "          6.2034e-01, -7.5479e-02, -3.6348e-01,  1.5472e+00, -3.5901e-01,\n",
       "          9.3755e-01, -1.5067e+00,  1.2774e+00, -1.0443e+00, -2.4424e+00,\n",
       "          5.5709e-01,  1.7321e+00,  5.4636e-02, -3.4457e-01,  1.7603e+00,\n",
       "          1.2434e+00,  6.7624e-02,  1.1873e+00,  1.6280e+00,  1.0769e-01,\n",
       "          2.0134e-01, -1.6246e-01,  2.1007e-03, -1.1351e+00,  2.2437e-01,\n",
       "         -4.6005e-01,  3.2747e-01,  6.2054e-01,  1.6853e-01, -5.6260e-01,\n",
       "         -7.7057e-01,  1.3820e+00,  7.2843e-01,  1.9859e+00,  1.9077e+00,\n",
       "         -8.7886e-01, -4.3094e-01,  1.5619e+00,  1.0658e+00,  1.0634e+00,\n",
       "          2.2776e-01, -4.6105e-01,  1.3709e+00, -1.1634e+00,  8.7547e-01,\n",
       "          1.1946e+00,  1.1209e+00,  1.0271e+00, -6.7899e-01, -2.0458e+00,\n",
       "         -2.1902e-01,  2.0669e-01,  5.6875e-01,  6.3425e-01,  1.3628e-01,\n",
       "         -3.2539e-01,  1.1104e+00, -5.7971e-01,  4.1337e-01, -4.7157e-01,\n",
       "         -9.6050e-01, -1.1842e+00, -5.7124e-01, -3.2206e-01,  1.2900e+00,\n",
       "         -3.4472e-01, -6.3878e-03,  2.8472e-01, -1.7083e+00, -2.2726e-01,\n",
       "         -6.8842e-01,  4.2970e-01,  3.5255e-01, -9.8703e-02, -8.6682e-02,\n",
       "         -3.3206e-01, -7.8557e-01, -3.1536e-01,  1.3444e-01, -3.7335e-01,\n",
       "         -6.6788e-01, -9.9956e-01,  2.5297e-01,  9.3248e-01, -3.9107e-01,\n",
       "          5.1666e-02, -5.5675e-01, -6.6877e-01,  2.6065e-02,  7.2971e-01,\n",
       "         -3.7042e-01, -2.2996e-01, -3.4134e-01, -9.5153e-02, -9.1251e-01,\n",
       "          4.9672e-02,  3.8930e-01, -3.5756e-01, -8.9463e-01, -1.2855e+00,\n",
       "         -3.8003e-01,  5.7523e-01, -3.4906e-01,  8.8763e-01,  9.8913e-02,\n",
       "         -1.5412e-01,  1.2170e+00, -7.5440e-01, -5.8133e-01, -2.2013e+00,\n",
       "          8.3681e-01, -1.8351e+00,  3.5133e-01,  9.9959e-02, -8.3073e-01,\n",
       "         -9.1256e-01,  3.4511e-01,  3.4896e-01, -6.3601e-01, -9.9304e-01,\n",
       "         -1.1932e+00, -2.4387e+00,  1.5163e+00, -3.2655e-01, -1.0208e+00,\n",
       "         -4.0876e-01, -1.5195e+00, -1.1040e+00, -2.2788e+00, -1.1042e+00,\n",
       "         -4.5359e-01, -8.5513e-02, -6.2637e-01,  1.3467e+00,  1.0761e+00]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "prediction = model(data)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah itu hitung loss function sebagai total perbedaan prediksi terhadap label dan lakukan backpropagation sekali\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1056.6899, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss = ((prediction - labels)**2).sum()\n",
    "print(loss)\n",
    "# backward pass\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunakan optimalisasi SGD dengan menggunakan learning rate 1e-7 dan momentum 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-7, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panggil ``.step()`` untuk menginisiasi gradient descent. Optimalisasi akan mengubah parameter sesuai dengan gradien yang telah tersimpan di ``.grad``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediksi dan hitung akurasi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(990.7310, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction = model(data)\n",
    "loss = ((prediction - labels)**2).sum()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coba sekarang bekukan semua parameter dari model tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediksi dan hitung akurasi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(935.0635)\n"
     ]
    }
   ],
   "source": [
    "prediction = model(data)\n",
    "loss = ((prediction - labels)**2).sum()\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bacaan Opsional: Gradien Tensor dan Produk Jacobian\n",
    "--------------------------------------\n",
    "\n",
    "Di banyak kasus, kita memiliki loss function skalar, dan perlu menghitung gradien terhadap suatu parameter. Tetapi ada kasus dimana fungsi output adalah sebuah tensor. Pada kasus ini, PyTorch mengijinkan perhitungan **Jacobian product**, dan bukan gradien tersebut.\n",
    "\n",
    "Pada sebuah fungsi vektor $\\vec{y}=f(\\vec{x})$, dimana\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ dan\n",
    "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$, sebuah gradien dari\n",
    "$\\vec{y}$ terhadap $\\vec{x}$ adalah **Jacobian\n",
    "matrix**:\n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\n",
    "\n",
    "Daripada menghitung matriks Jacobian tersebut, PyTorch mengijinkan perhitungan **Jacobian Product** $v^T\\cdot J$ untuk sebuah vektor input $v=(v_1 \\dots v_m)$. Ini dapat dicapai dengan memanggil ``backward`` dengan $v$ sebagai argumen. Ukuran $v$ harus sama dengan ukuran tensor awal, yang terhadapnya kita ingin menghitung produk:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan ketika kita memanggil ``backward`` untuk kedua kali dengan argumen yang sama, nilai dari gradien berbeda. Ini terjadi karena ketika melakukan propagasi ``backward``, PyTorch **mengakumulasi gradien**, dimana nilai gradien yang terhitung ditambahkan kepada properti ``grad`` dari semua leaf nodes dari computational graph. Jika kamu ingin menghitung gradien sebenarnya, kamu perlu mengosongkan properti ``grad`` sebelumnya. Di dalam pelatihan, sebuah *optimizer* membantu kita untuk melakukan hal ini.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Catatan</h4><p>Sebelumnya kita memanggil fungsi ``backward()`` tanpa parameter. Ini ekuivalen dengan memanggil ``backward(torch.tensor(1.0))``, dimana ini cara yang efektif untuk menghitung gradien dalam kasus fungsi skalar, seperti loss dalam pelatihan neural network\n",
    "</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
